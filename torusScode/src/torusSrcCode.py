-ai
      /vortex_code_model/vortex_code.py")
  ‚éø #!/usr/bin/env python3
    """
    Vortex Code - AI Code Generation with Torus Geometry
    """

    import json
    import torch
    from pathlib import Path
    import sys
    import re

    sys.path.append('../src')

    from advanced_tokenizer import BPETokenizer
    from torus_language_model import TorusFractalLM

    class VortexCodeGenerator:
        """Vortex Code - Fast code generation with geometric 
    understanding"""

        def __init__(self, model_path="."):
            self.model_path = Path(model_path)
            self.device = torch.device('cpu')

            # Load model and tokenizer
            self.model = self._load_model()
            self.tokenizer = self._load_tokenizer()

            # Load code templates
            with open(self.model_path / "vortex_code_templates.json",
    'r') as f:
                self.templates = json.load(f)

            print("üåÄ Vortex Code initialized")
            print(f"   Vocabulary: {len(self.tokenizer.vocab)} tokens")
            print(f"   Templates: {len(self.templates)} patterns")

        def _load_model(self):
            with open(self.model_path / "config.json", 'r') as f:
                config = json.load(f)

            model = TorusFractalLM(
                vocab_size=config['vocab_size'],
                embed_dim=config['embed_dim'],
                num_heads=config['num_heads'],
                num_layers=config['num_layers'],
                ff_dim=config['ff_dim'],
                max_seq_length=config['max_seq_length'],
                R=config['R'],
                r=config['r'],
                num_lattice_levels=config['num_lattice_levels'],
                dropout=0.0
            ).to(self.device)

            checkpoint = torch.load(
                self.model_path / "model.pt",
                map_location=self.device,
                weights_only=False
            )
            model.load_state_dict(checkpoint['model_state_dict'],
    strict=False)
            model.eval()

            return model

        def _load_tokenizer(self):
            tokenizer = BPETokenizer()
            tokenizer.load(self.model_path / "tokenizer.json")
            return tokenizer

        def generate_code(self, prompt, max_tokens=512):
            """Generate code using Vortex dynamics"""
            # First, check templates
            prompt_lower = prompt.lower()
            for key, template in self.templates.items():
                if key in prompt_lower:
                    return template

            # Generate with vortex-enhanced model
            if "def " in prompt:
                return self._complete_code(prompt, max_tokens)

            # Generate from scratch
            code_prompt = f"# {prompt}\ndef solution"
            return self._complete_code(code_prompt, max_tokens)

        @torch.no_grad()
        def _complete_code(self, prompt, max_tokens):
            """Complete code with vortex dynamics"""
            # Tokenize
            tokens = self.tokenizer.tokenize(prompt)
            token_ids = [self.tokenizer.vocab.get(t,
    self.tokenizer.vocab.get('<UNK>', 0)) for t in tokens]
            positions = self.tokenizer.get_positions(tokens)

            generated = token_ids.copy()

            for _ in range(max_tokens):
                input_ids = torch.tensor([generated[-512:]])
                pos_tensor = torch.tensor([[[p.u, p.v] for p in
    positions[-512:]]])

                outputs = self.model(input_ids, positions=pos_tensor)
                logits = outputs['logits'][0, -1, :]

                # Apply vortex-inspired sampling
                logits = logits / 0.8
                top_k = 50
                top_values, top_indices = torch.topk(logits, min(top_k,
    logits.size(-1)))
                probs = torch.softmax(top_values, dim=-1)

                next_token_idx = torch.multinomial(probs, 1).item()
                next_token = top_indices[next_token_idx].item()

                if next_token == self.tokenizer.vocab.get('<END>', 1):
                    break

                generated.append(next_token)

                # Update positions with vortex flow
                inv_vocab = {v: k for k, v in
    self.tokenizer.vocab.items()}
                new_token_str = inv_vocab.get(next_token, '<UNK>')
                if new_token_str != '<UNK>':
                    new_pos =
    self.tokenizer.get_positions([new_token_str])[0]
                    positions.append(new_pos)

            # Decode
            inv_vocab = {v: k for k, v in self.tokenizer.vocab.items()}
            code = ''
            for token_id in generated:
                token = inv_vocab.get(token_id, '')
                if token and token not in ['<UNK>', '<PAD>', '<END>']:
                    code += token + ' '

            # Clean up code formatting
            code = self._format_code(code)
            return code

        def _format_code(self, code):
            """Format generated code properly"""
            # Fix spacing around operators and symbols
            code = re.sub(r'\s*([()\[\]{}:;,])\s*', r'\1', code)
            code = re.sub(r'\s*([=+\-*/<>!]+)\s*', r' \1 ', code)
            code = code.replace('( ', '(').replace(' )', ')')
            code = code.replace('[ ', '[').replace(' ]', ']')
            code = code.replace('{ ', '{').replace(' }', '}')
            code = code.replace(': ', ':')
            code = code.replace(' :', ':')
            code = code.replace(' ,', ',')

            # Fix newlines and indentation
            code = code.replace('\\n', '\n')
            code = code.replace('\\t', '    ')

            return code.strip()

    def demo():
        """Demo Vortex Code generation"""
        print("üåÄ Vortex Code Demo")
        print("="*60)

        generator = VortexCodeGenerator()

        test_prompts = [
            "Write a function to calculate factorial",
            "def add(a, b):",
            "Implement binary search",
            "Create a Stack class"
        ]

        for prompt in test_prompts:
            print(f"\nPrompt: {prompt}")
            code = generator.generate_code(prompt)
            print(f"Generated:\n{code}")
            print("-"*40)

    if __name__ == "__main__":
        demo()


‚è∫ Here's the complete Vortex Code source code. This is the main generator
   that uses:

  1. Torus Geometry Model - Uses torus mathematical properties for code
  generation
  2. BPE Tokenizer - Advanced tokenization system
  3. Vortex Dynamics - Special sampling strategy (line 101-105)
  4. Template System - Fast lookup for common patterns

  The key components are:
  - TorusFractalLM - The main language model with torus geometry
  - BPETokenizer - Byte-pair encoding tokenizer
  - vortex_code_templates.json - Pre-defined code templates
  - Position tracking with (u, v) coordinates on the torus surface

  The simplified version I created (vortex_code_simple.py) removes the ML
   dependencies and just uses the template system for easier deployment.

  Would you like me to also show you the dependencies
  (advanced_tokenizer.py and torus_language_model.py)?

